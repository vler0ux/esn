{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des causes d'attrition chez TechNova Partners\n",
    "## √âtape 3 : Feature Engineering - Pr√©paration des donn√©es pour la mod√©lisation\n",
    "\n",
    "**Objectif** : Pr√©parer les donn√©es (X et y) pour l'entra√Ænement de mod√®les de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#biblioth√®que Python\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "# 1. Importation des biblioth√®ques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "#biblioth√®que Python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es avec succ√®s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement du Dataset de l'analyse Exploratoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Chargement des donn√©es compl√®tes\n",
    "df = pd.read_csv('data_complete.csv')\n",
    "\n",
    "print(f\"üìä Dataset charg√© : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Affichage de variables quantitatives et qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_quantitatives =df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "variables_qualitatives = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"üî¢ VARIABLES QUANTITATIVES (num√©riques) :\\n\")\n",
    "for i, col in enumerate(variables_quantitatives, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nüìù VARIABLES QUALITATIVES (cat√©gorielles) :\\n\")\n",
    "for i, col in enumerate(variables_qualitatives, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 .Encoding des variables qualitative , methode : OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Analyse des variables cat√©gorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 1 : ANALYSE DES VARIABLES CAT√âGORIELLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Lister toutes les variables cat√©gorielles (object)\n",
    "variables_qualitatives = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nüìù Variables cat√©gorielles d√©tect√©es : {len(variables_qualitatives)}\\n\")\n",
    "\n",
    "# Analyser chaque variable\n",
    "for col in variables_qualitatives:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìå Variable : {col}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Valeurs uniques\n",
    "    valeurs_uniques = df[col].dropna().unique()\n",
    "    nb_valeurs = len(valeurs_uniques)\n",
    "    \n",
    "    print(f\"Nombre de valeurs uniques : {nb_valeurs}\")\n",
    "    print(f\"Valeurs : {list(valeurs_uniques)}\")\n",
    "    \n",
    "    # Distribution\n",
    "    print(f\"\\nDistribution :\")\n",
    "    distribution = df[col].value_counts()\n",
    "    for val, count in distribution.items():\n",
    "        pourcentage = (count / len(df)) * 100\n",
    "        print(f\"  - {val:30s} : {count:4d} ({pourcentage:5.2f}%)\")\n",
    "    \n",
    "    # Valeurs manquantes\n",
    "    nb_missing = df[col].isnull().sum()\n",
    "    if nb_missing > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Valeurs manquantes : {nb_missing} ({(nb_missing/len(df)*100):.2f}%)\")\n",
    "    else :\n",
    "        print(\"pas de valeurs manquantes sur l'ensemble de Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "On a 8 variables cat√©gorielles.  \n",
    "On observe aucune valeur aberrante.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Encodage des variables cat√©gorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a/ encodage de la variable 'ordre_deplacement' en OrdinaleEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√âTAPE 2.2 : ENCODAGE DES VARIABLES ORDINALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# D√©finir les ordres logiques pour chaque variable ordinale\n",
    "\n",
    "# 1. Fr√©quence de d√©placement\n",
    "ordre_deplacement = ['Aucun', 'Occasionnel', 'Frequent']\n",
    "\n",
    "print(\"\\nüìä Variables ordinales √† encoder :\\n\")\n",
    "print(\"1. frequence_deplacement\")\n",
    "print(f\"   Ordre : {' < '.join(ordre_deplacement)}\")\n",
    "print(f\"   Encodage : {dict(zip(ordre_deplacement, range(len(ordre_deplacement))))}\")\n",
    "\n",
    "\n",
    "# V√©rifier que toutes les valeurs sont bien pr√©sentes\n",
    "print(\"\\nüîç V√©rification des valeurs :\")\n",
    "print(f\"\\nfrequence_deplacement - Valeurs dans les donn√©es :\")\n",
    "print(df['frequence_deplacement'].unique())\n",
    "\n",
    "\n",
    "# Encoder fr√©quence de d√©placement\n",
    "encoder_deplacement = OrdinalEncoder(categories=[ordre_deplacement])\n",
    "df['frequence_deplacement_encoded'] = encoder_deplacement.fit_transform(\n",
    "    df[['frequence_deplacement']]\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Encodage ordinal termin√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b/ Encodage simple\n",
    "Pour les variables qui ont 2 cat√©gories :\n",
    "- genre\n",
    "- heures suppl√©mentaire\n",
    "- a_quitte_l_entreprise (variable cible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# √âTAPE 2.4 : ENCODAGE DES VARIABLES BINAIRES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√âTAPE 2.4 : ENCODAGE DES VARIABLES BINAIRES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. heure_supplementaires : Oui ‚Üí 1, Non ‚Üí 0\n",
    "df['heure_supplementaires_encoded'] = (df['heure_supplementaires'] == 'Oui').astype(int)\n",
    "df['target'] = (df['a_quitte_l_entreprise'] == 'Oui').astype(int)\n",
    "df['genre_encoded'] = (df['genre'] == 'M').astype(int)\n",
    "\n",
    "print(\"\\n‚úÖ Variables binaires encod√©es :\")\n",
    "print(df[['heure_supplementaires', 'heure_supplementaires_encoded']].value_counts().sort_index())\n",
    "print(df[['a_quitte_l_entreprise', 'target']].value_counts().sort_index())\n",
    "print(df[['genre', 'genre_encoded']].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b/ Encodage des autres variables OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√âTAPE 2.3 : ENCODAGE DES VARIABLES NOMINALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Variables nominales √† encoder\n",
    "variables_nominales = [\n",
    "    'statut_marital',\n",
    "    'departement',\n",
    "    'poste',\n",
    "    'domaine_etude'\n",
    "]\n",
    "\n",
    "print(f\"\\nüìù Variables nominales √† encoder : {len(variables_nominales)}\\n\")\n",
    "for var in variables_nominales:\n",
    "    nb_categories = df[var].nunique()\n",
    "    print(f\"  - {var:25s} : {nb_categories} cat√©gories\")\n",
    "\n",
    "# Cr√©er l'encoder avec drop='first' pour √©viter la multicolin√©arit√©\n",
    "encoder_nominal = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "# Encoder\n",
    "encoded_array = encoder_nominal.fit_transform(df[variables_nominales])\n",
    "\n",
    "# R√©cup√©rer les noms des colonnes\n",
    "feature_names = encoder_nominal.get_feature_names_out(variables_nominales)\n",
    "\n",
    "# Cr√©er un DataFrame avec les variables encod√©es\n",
    "df_encoded = pd.DataFrame(\n",
    "    encoded_array, \n",
    "    columns=feature_names,\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ OneHotEncoding termin√© !\")\n",
    "print(f\"   Nombre de nouvelles colonnes cr√©√©es : {df_encoded.shape[1]}\")\n",
    "print(f\"\\nüìã Aper√ßu des colonnes cr√©√©es :\")\n",
    "for i, col in enumerate(df_encoded.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cr√©ation de la variable cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (df['target'])\n",
    "reste = (y == 0).sum()\n",
    "parti = (y == 1).sum()\n",
    "pct_parti= parti/len(y)*100\n",
    "pct_reste = reste/len(y)*100\n",
    "\n",
    "print(\"\\n‚úÖ Variable cible 'y' cr√©√©e avec succ√®s\")\n",
    "print(f\"\\nType : {type(y)}\")\n",
    "print(f\"Shape : {y.shape}\")\n",
    "print(f\"\\nüìä Distribution de y :\")\n",
    "print(f\"  - Rest√©s (0) : {pct_reste:.2f}%\")\n",
    "print(f\"  - Partis (1) : {pct_parti:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "On voit qu'il y a une disparit√© entre les valeurs de la Target avec une pr√©pond√©rance pour \"rest√©s\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse des variables QUANTITATIVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Exclusion des features non utilis√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 3 : CR√âATION DU DATAFRAME X\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Lister les colonnes √† EXCLURE de X\n",
    "colonnes_a_exclure = [\n",
    "    'id_employee',                          # Identifiant (pas pr√©dictif)\n",
    "    'a_quitte_l_entreprise',                # Variable cible\n",
    "    \n",
    "    # Variables cat√©gorielles ORIGINALES (avant encodage)\n",
    "    'genre',\n",
    "    'statut_marital',\n",
    "    'departement',\n",
    "    'poste',\n",
    "    'domaine_etude',\n",
    "    'heure_supplementaires',\n",
    "    'frequence_deplacement',\n",
    "\n",
    "    #la target\n",
    "    'target',\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"\\n‚ùå Colonnes √† EXCLURE de X ({len(colonnes_a_exclure)}) :\")\n",
    "for col in colonnes_a_exclure:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "# 2. Cr√©er X en concat√©nant :\n",
    "#    - Variables num√©riques (sauf celles √† exclure)\n",
    "#    - Variables encod√©es (ordinales + nominales + binaires)\n",
    "\n",
    "# Variables num√©riques √† garder\n",
    "variables_numeriques = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "variables_numeriques_a_garder = [col for col in variables_numeriques \n",
    "                                  if col not in colonnes_a_exclure]\n",
    "\n",
    "print(f\"\\n‚úÖ Variables NUM√âRIQUES √† garder ({len(variables_numeriques_a_garder)}) :\")\n",
    "for i, col in enumerate(variables_numeriques_a_garder, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Cr√©er X_numeriques\n",
    "X_numeriques = df[variables_numeriques_a_garder]\n",
    "\n",
    "print(f\"\\nüìä X_numeriques : {X_numeriques.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Fusion des features pour nouveau Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat√©ner toutes les features\n",
    "X = pd.concat([\n",
    "    X_numeriques,           # Variables num√©riques originales\n",
    "    df_encoded,             # Variables nominales encod√©es (OneHot)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\n‚úÖ DataFrame X cr√©√© avec succ√®s !\")\n",
    "print(f\"\\nüìä Dimensions de X : {X.shape}\")\n",
    "print(f\"   - {X.shape[0]} lignes (employ√©s)\")\n",
    "print(f\"   - {X.shape[1]} colonnes (features)\")\n",
    "\n",
    "display(X.columns.tolist()) \n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "On a bien exclus les features cat√©gorielles originales et la target\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Matrice de Corr√©lation de Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# √âTAPE 4.2 : VISUALISATION DE LA MATRICE DE CORR√âLATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALISATION DE LA MATRICE DE CORR√âLATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "correlation_matrix = X.corr(method='pearson')\n",
    "\n",
    "# Cr√©er la heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    annot=False,           # Pas d'annotation (trop de features)\n",
    "    fmt='.2f', \n",
    "    cmap='RdYlBu_r',       # Rouge = corr√©lation positive, Bleu = n√©gative\n",
    "    center=0,              # Centre sur 0\n",
    "    square=True, \n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "plt.title('Matrice de corr√©lation de Pearson (toutes les features)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation des couleurs :\")\n",
    "print(\"   üî¥ Rouge fonc√©   : Corr√©lation positive forte (proche de +1)\")\n",
    "print(\"   üîµ Bleu fonc√©    : Corr√©lation n√©gative forte (proche de -1)\")\n",
    "print(\"   ‚ö™ Blanc/Jaune   : Pas de corr√©lation (proche de 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_spearman = X.corr(method='spearman')\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(\n",
    "    correlation_spearman, \n",
    "    annot=True,           # Afficher les valeurs\n",
    "    fmt='.2f',            # Format 2 d√©cimales\n",
    "    cmap='coolwarm',      # Palette de couleurs (bleu=n√©gatif, rouge=positif)\n",
    "    center=0,             # Centrer sur 0\n",
    "    vmin=-1, vmax=1,      # √âchelle de -1 √† 1\n",
    "    square=True,          # Cases carr√©es\n",
    "    linewidths=0.5,       # Lignes entre cases\n",
    "    cbar_kws={\"shrink\": 0.8}  # Taille de la barre de couleur\n",
    ")\n",
    "plt.title('Matrice de Corr√©lation de Spearman', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identification des corr√©lations fortes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# √âTAPE 4.3 : IDENTIFICATION DES CORR√âLATIONS FORTES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IDENTIFICATION DES PAIRES FORTEMENT CORR√âL√âES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Seuil de corr√©lation √† consid√©rer comme \"forte\"\n",
    "seuil = 0.7\n",
    "\n",
    "# Trouver les paires de features fortement corr√©l√©es\n",
    "correlations_fortes = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        \n",
    "        if abs(corr_value) >= seuil:\n",
    "            correlations_fortes.append({\n",
    "                'Feature_1': correlation_matrix.columns[i],\n",
    "                'Feature_2': correlation_matrix.columns[j],\n",
    "                'Correlation': corr_value\n",
    "            })\n",
    "\n",
    "# Cr√©er un DataFrame pour affichage\n",
    "df_corr_fortes = pd.DataFrame(correlations_fortes)\n",
    "\n",
    "if len(df_corr_fortes) > 0:\n",
    "    # Trier par valeur absolue de corr√©lation (d√©croissant)\n",
    "    df_corr_fortes['Abs_Correlation'] = df_corr_fortes['Correlation'].abs()\n",
    "    df_corr_fortes = df_corr_fortes.sort_values('Abs_Correlation', ascending=False)\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  {len(df_corr_fortes)} paires de features avec corr√©lation ‚â• {seuil} :\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    \n",
    "    # Afficher le DataFrame\n",
    "    display(df_corr_fortes[['Feature_1', 'Feature_2', 'Correlation']].reset_index(drop=True))\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚úÖ Aucune paire de features avec corr√©lation ‚â• {seuil}\")\n",
    "    print(\"   ‚Üí Pas de multicolin√©arit√© importante d√©tect√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyse des corr√©lations moyennes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# √âTAPE 4.4 : CORR√âLATIONS MOYENNES (seuil 0.5 - 0.8)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CORR√âLATIONS MOYENNES (0.5 ‚â§ |r| < 0.8)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Trouver les paires avec corr√©lation moyenne\n",
    "seuil_min = 0.5\n",
    "seuil_max = 0.8\n",
    "\n",
    "correlations_moyennes = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        \n",
    "        if seuil_min <= abs(corr_value) < seuil_max:\n",
    "            correlations_moyennes.append({\n",
    "                'Feature_1': correlation_matrix.columns[i],\n",
    "                'Feature_2': correlation_matrix.columns[j],\n",
    "                'Correlation': corr_value\n",
    "            })\n",
    "\n",
    "df_corr_moyennes = pd.DataFrame(correlations_moyennes)\n",
    "\n",
    "if len(df_corr_moyennes) > 0:\n",
    "    df_corr_moyennes['Abs_Correlation'] = df_corr_moyennes['Correlation'].abs()\n",
    "    df_corr_moyennes = df_corr_moyennes.sort_values('Abs_Correlation', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä {len(df_corr_moyennes)} paires avec corr√©lation moyenne ({seuil_min} ‚â§ |r| < {seuil_max})\")\n",
    "    \n",
    "    # Afficher les 10 plus fortes\n",
    "    print(f\"\\nüîù Top 10 des corr√©lations moyennes les plus fortes :\")\n",
    "    print(\"=\"*80)\n",
    "    display(df_corr_moyennes[['Feature_1', 'Feature_2', 'Correlation']].head(10).reset_index(drop=True))\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Aucune corr√©lation moyenne d√©tect√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mod√©lisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# √âTAPE 3.1 : S√âPARATION TRAIN/TEST\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 3.1 : S√âPARATION TRAIN/TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# V√©rifier qu'on a bien X et y\n",
    "print(f\"\\nüìä V√©rification des donn√©es :\")\n",
    "print(f\"  ‚Ä¢ X (features) : {X.shape}\")\n",
    "print(f\"  ‚Ä¢ y (target) : {y.shape}\")\n",
    "print(f\"  ‚Ä¢ M√™me nombre de lignes ? {X.shape[0] == y.shape[0]} ‚úì\")\n",
    "\n",
    "# Distribution de y\n",
    "print(f\"\\nüìä Distribution de la variable cible y :\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\n  ‚Ä¢ Classe 0 (Rest√©s) : {(y==0).sum()} employ√©s ({(y==0).sum()/len(y)*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Classe 1 (Partis) : {(y==1).sum()} employ√©s ({(y==1).sum()/len(y)*100:.2f}%)\")\n",
    "\n",
    "ratio_desequilibre = (y==0).sum() / (y==1).sum()\n",
    "print(f\"  ‚Ä¢ Ratio de d√©s√©quilibre : {ratio_desequilibre:.1f}:1\")\n",
    "\n",
    "# S√©paration train/test\n",
    "print(f\"\\n‚è≥ S√©paration des donn√©es en cours...\")\n",
    "print(f\"  ‚Ä¢ 80% pour l'entra√Ænement (TRAIN)\")\n",
    "print(f\"  ‚Ä¢ 20% pour le test (TEST)\")\n",
    "print(f\"  ‚Ä¢ Stratification : OUI (pour garder les m√™mes proportions)\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,                      # Features\n",
    "    y,                      # Target\n",
    "    test_size=0.2,          # 20% pour le test\n",
    "    random_state=42,        # Pour la reproductibilit√©\n",
    "    stratify=y              # IMPORTANT : garde la m√™me proportion 0/1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ S√©paration effectu√©e avec succ√®s !\")\n",
    "\n",
    "# Afficher les dimensions\n",
    "print(f\"\\nüìä Dimensions des jeux de donn√©es :\")\n",
    "print(f\"{'‚îÄ'*80}\")\n",
    "print(f\"  JEU D'ENTRA√éNEMENT (TRAIN) :\")\n",
    "print(f\"    ‚Ä¢ X_train : {X_train.shape} ‚Üí {X_train.shape[0]} employ√©s, {X_train.shape[1]} features\")\n",
    "print(f\"    ‚Ä¢ y_train : {y_train.shape}\")\n",
    "print(f\"\\n  JEU DE TEST (TEST) :\")\n",
    "print(f\"    ‚Ä¢ X_test  : {X_test.shape} ‚Üí {X_test.shape[0]} employ√©s, {X_test.shape[1]} features\")\n",
    "print(f\"    ‚Ä¢ y_test  : {y_test.shape}\")\n",
    "\n",
    "# V√©rifier la stratification (tr√®s important !)\n",
    "print(f\"\\n‚úÖ V√©rification de la STRATIFICATION :\")\n",
    "print(f\"{'‚îÄ'*80}\")\n",
    "\n",
    "print(f\"\\n  TRAIN :\")\n",
    "print(f\"    ‚Ä¢ Rest√©s (0) : {(y_train==0).sum()} ({(y_train==0).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"    ‚Ä¢ Partis (1) : {(y_train==1).sum()} ({(y_train==1).sum()/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  TEST :\")\n",
    "print(f\"    ‚Ä¢ Rest√©s (0) : {(y_test==0).sum()} ({(y_test==0).sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"    ‚Ä¢ Partis (1) : {(y_test==1).sum()} ({(y_test==1).sum()/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  ORIGINAL :\")\n",
    "print(f\"    ‚Ä¢ Rest√©s (0) : {(y==0).sum()} ({(y==0).sum()/len(y)*100:.2f}%)\")\n",
    "print(f\"    ‚Ä¢ Partis (1) : {(y==1).sum()} ({(y==1).sum()/len(y)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüí° Les proportions sont identiques dans train, test et original ‚Üí Stratification OK !\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ √âTAPE 3.1 TERMIN√âE : Donn√©es pr√™tes pour la mod√©lisation\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PIPELINE COMPLET : TEST DE PLUSIEURS MOD√àLES\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE DE MOD√âLISATION - TEST DE PLUSIEURS ALGORITHMES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. D√âFINIR LES MOD√àLES √Ä TESTER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìã Mod√®les √† tester :\")\n",
    "print(\"‚îÄ\"*80)\n",
    "\n",
    "modeles = {\n",
    "    'Dummy (Baseline)': DummyClassifier(strategy='most_frequent', random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "for i, nom in enumerate(modeles.keys(), 1):\n",
    "    print(f\"  {i}. {nom}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total : {len(modeles)} mod√®les\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ENTRA√éNER ET √âVALUER TOUS LES MOD√àLES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENTRA√éNEMENT ET √âVALUATION DES MOD√àLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "resultats = []\n",
    "matrices_confusion = {}\n",
    "predictions = {}\n",
    "\n",
    "for nom_modele, modele in modeles.items():\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"üîÑ Mod√®le en cours : {nom_modele}\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    print(\"  ‚è≥ Entra√Ænement...\")\n",
    "    modele.fit(X_train, y_train)\n",
    "    print(\"  ‚úÖ Entra√Æn√©\")\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    print(\"  ‚è≥ Pr√©dictions...\")\n",
    "    y_pred_train = modele.predict(X_train)\n",
    "    y_pred_test = modele.predict(X_test)\n",
    "    \n",
    "    # Probabilit√©s (pour ROC-AUC)\n",
    "    try:\n",
    "        y_proba_test = modele.predict_proba(X_test)[:, 1]\n",
    "    except:\n",
    "        y_proba_test = y_pred_test  # Pour Dummy qui n'a pas predict_proba\n",
    "    \n",
    "    # M√©triques sur TRAIN\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    # M√©triques sur TEST\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test, zero_division=0)\n",
    "    recall_test = recall_score(y_test, y_pred_test, zero_division=0)\n",
    "    f1_test = f1_score(y_test, y_pred_test, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        roc_auc_test = roc_auc_score(y_test, y_proba_test)\n",
    "    except:\n",
    "        roc_auc_test = 0.5  # Score al√©atoire pour Dummy\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    matrices_confusion[nom_modele] = cm\n",
    "    predictions[nom_modele] = y_pred_test\n",
    "    \n",
    "    # Calculer overfitting\n",
    "    overfitting = acc_train - acc_test\n",
    "    \n",
    "    # Stocker les r√©sultats\n",
    "    resultats.append({\n",
    "        'Mod√®le': nom_modele,\n",
    "        'Accuracy_train': acc_train,\n",
    "        'Accuracy_test': acc_test,\n",
    "        'Precision': precision_test,\n",
    "        'Recall': recall_test,\n",
    "        'F1-Score': f1_test,\n",
    "        'ROC-AUC': roc_auc_test,\n",
    "        'Overfitting': overfitting\n",
    "    })\n",
    "    \n",
    "    # Afficher r√©sum√©\n",
    "    print(f\"  ‚úÖ Termin√©\")\n",
    "    print(f\"     ‚Ä¢ Accuracy (test) : {acc_test:.4f}\")\n",
    "    print(f\"     ‚Ä¢ Recall (test) : {recall_test:.4f}\")\n",
    "    print(f\"     ‚Ä¢ F1-Score (test) : {f1_test:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TOUS LES MOD√àLES ONT √âT√â ENTRA√éN√âS ET √âVALU√âS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TABLEAU COMPARATIF DES PERFORMANCES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä TABLEAU COMPARATIF DES PERFORMANCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_resultats = pd.DataFrame(resultats)\n",
    "\n",
    "# Trier par F1-Score (m√©trique √©quilibr√©e)\n",
    "df_resultats = df_resultats.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + df_resultats.to_string(index=False))\n",
    "\n",
    "# Identifier le meilleur mod√®le\n",
    "meilleur_modele = df_resultats.iloc[0]['Mod√®le']\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE (F1-Score) : {meilleur_modele}\")\n",
    "\n",
    "# Identifier le mod√®le avec le meilleur Recall\n",
    "df_resultats_recall = df_resultats.sort_values('Recall', ascending=False)\n",
    "meilleur_recall = df_resultats_recall.iloc[0]['Mod√®le']\n",
    "print(f\"üéØ MEILLEUR RECALL : {meilleur_recall} (Recall = {df_resultats_recall.iloc[0]['Recall']:.4f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VISUALISATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä G√âN√âRATION DES VISUALISATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# GRAPHIQUE 1 : Comparaison des m√©triques principales\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy\n",
    "df_resultats.plot(x='Mod√®le', y='Accuracy_test', kind='barh', ax=axes[0, 0], \n",
    "                  color='steelblue', legend=False)\n",
    "axes[0, 0].set_title('Accuracy (Test)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Score', fontsize=12)\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "df_resultats.plot(x='Mod√®le', y='Precision', kind='barh', ax=axes[0, 1], \n",
    "                  color='green', legend=False)\n",
    "axes[0, 1].set_title('Precision (Test)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Score', fontsize=12)\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "df_resultats.plot(x='Mod√®le', y='Recall', kind='barh', ax=axes[1, 0], \n",
    "                  color='orange', legend=False)\n",
    "axes[1, 0].set_title('Recall (Test) - D√©tection des d√©parts', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Score', fontsize=12)\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# F1-Score\n",
    "df_resultats.plot(x='Mod√®le', y='F1-Score', kind='barh', ax=axes[1, 1], \n",
    "                  color='purple', legend=False)\n",
    "axes[1, 1].set_title('F1-Score (Test)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Score', fontsize=12)\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comparaison des performances des mod√®les', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique 1/3 : M√©triques principales\")\n",
    "\n",
    "# GRAPHIQUE 2 : Overfitting (Train vs Test)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(df_resultats))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, df_resultats['Accuracy_train'], width, \n",
    "               label='Train', color='lightblue', edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, df_resultats['Accuracy_test'], width, \n",
    "               label='Test', color='coral', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Mod√®les', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Overfitting : Accuracy Train vs Test', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_resultats['Mod√®le'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique 2/3 : Overfitting (Train vs Test)\")\n",
    "\n",
    "# GRAPHIQUE 3 : Matrices de confusion\n",
    "n_modeles = len(modeles)\n",
    "n_cols = 3\n",
    "n_rows = (n_modeles + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (nom_modele, cm) in enumerate(matrices_confusion.items()):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                ax=axes[idx], cbar=False,\n",
    "                xticklabels=['Rest√©', 'Parti'],\n",
    "                yticklabels=['Rest√©', 'Parti'])\n",
    "    axes[idx].set_title(f'{nom_modele}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Pr√©dit', fontsize=10)\n",
    "    axes[idx].set_ylabel('R√©el', fontsize=10)\n",
    "\n",
    "# Masquer les subplots vides\n",
    "for idx in range(len(modeles), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Matrices de confusion - Tous les mod√®les', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique 3/3 : Matrices de confusion\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ANALYSE D√âTAILL√âE DU MEILLEUR MOD√àLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üèÜ ANALYSE D√âTAILL√âE : {meilleur_modele}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Retrouver les pr√©dictions du meilleur mod√®le\n",
    "y_pred_best = predictions[meilleur_modele]\n",
    "cm_best = matrices_confusion[meilleur_modele]\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìã Classification Report :\")\n",
    "print(\"‚îÄ\"*80)\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['Rest√© (0)', 'Parti (1)'],\n",
    "                          digits=4))\n",
    "\n",
    "# D√©tail de la matrice de confusion\n",
    "print(\"üìä Matrice de confusion d√©taill√©e :\")\n",
    "print(\"‚îÄ\"*80)\n",
    "print(f\"  TN (Rest√©s bien pr√©dits) : {cm_best[0,0]}\")\n",
    "print(f\"  FP (Fausses alertes) : {cm_best[0,1]}\")\n",
    "print(f\"  FN (D√©parts manqu√©s) : {cm_best[1,0]}\")\n",
    "print(f\"  TP (D√©parts d√©tect√©s) : {cm_best[1,1]}\")\n",
    "\n",
    "# Interpr√©tation m√©tier\n",
    "meilleur_row = df_resultats[df_resultats['Mod√®le'] == meilleur_modele].iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° INTERPR√âTATION POUR TECHNOVA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ Le mod√®le {meilleur_modele} est le plus performant :\")\n",
    "print(f\"   ‚Ä¢ F1-Score : {meilleur_row['F1-Score']:.2%}\")\n",
    "print(f\"   ‚Ä¢ Recall : {meilleur_row['Recall']:.2%} ‚Üí D√©tecte {meilleur_row['Recall']:.0%} des d√©parts\")\n",
    "print(f\"   ‚Ä¢ Precision : {meilleur_row['Precision']:.2%} ‚Üí {meilleur_row['Precision']:.0%} des alertes sont vraies\")\n",
    "\n",
    "print(f\"\\nüìä Sur {(y_test==1).sum()} employ√©s qui sont partis :\")\n",
    "print(f\"   ‚Ä¢ {cm_best[1,1]} ont √©t√© D√âTECT√âS (TP)\")\n",
    "print(f\"   ‚Ä¢ {cm_best[1,0]} ont √©t√© MANQU√âS (FN)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Overfitting : {meilleur_row['Overfitting']:.4f}\")\n",
    "if meilleur_row['Overfitting'] > 0.1:\n",
    "    print(\"     ‚Üí Attention : le mod√®le surappprend sur les donn√©es d'entra√Ænement\")\n",
    "else:\n",
    "    print(\"     ‚Üí OK : le mod√®le g√©n√©ralise bien\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PIPELINE TERMIN√â\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 6. SAUVEGARDE DES R√âSULTATS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüíæ Sauvegarde des r√©sultats...\")\n",
    "\n",
    "# Sauvegarder le DataFrame des r√©sultats\n",
    "df_resultats.to_csv('resultats_modeles.csv', index=False)\n",
    "print(\"‚úÖ R√©sultats sauvegard√©s dans 'resultats_modeles.csv'\")\n",
    "\n",
    "print(\"\\nüéØ PROCHAINES √âTAPES :\")\n",
    "print(\"  1. Optimiser le meilleur mod√®le (hyperparam√®tres)\")\n",
    "print(\"  2. G√©rer le d√©s√©quilibre des classes (class_weight, SMOTE)\")\n",
    "print(\"  3. Utiliser SHAP pour l'interpr√©tabilit√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 optimisation avec le Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# √âTAPE 4.1 : OPTIMISATION AVEC CLASS_WEIGHT\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score, f1_score\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 4.1 : OPTIMISATION - GESTION DU D√âS√âQUILIBRE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüí° Probl√®me actuel :\")\n",
    "print(\"  ‚Ä¢ Le mod√®le est d√©s√©quilibr√© : 84% rest√©s vs 16% partis\")\n",
    "print(\"  ‚Ä¢ Il favorise la classe majoritaire (Rest√©s)\")\n",
    "print(\"  ‚Ä¢ R√©sultat : bon Accuracy mais mauvais Recall\")\n",
    "\n",
    "print(\"\\nüéØ Solution : class_weight='balanced'\")\n",
    "print(\"  ‚Ä¢ Donne plus d'importance √† la classe minoritaire (Partis)\")\n",
    "print(\"  ‚Ä¢ Force le mod√®le √† mieux d√©tecter les d√©parts\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOGISTIC REGRESSION avec class_weight\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"‚îÄ\"*80)\n",
    "print(\"üîÑ Mod√®le 1 : Logistic Regression + class_weight='balanced'\")\n",
    "print(\"‚îÄ\"*80)\n",
    "\n",
    "log_balanced = LogisticRegression(\n",
    "    max_iter=1000, \n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # ‚Üê IMPORTANT\n",
    ")\n",
    "\n",
    "log_balanced.fit(X_train, y_train)\n",
    "y_pred_log_balanced = log_balanced.predict(X_test)\n",
    "\n",
    "print(\"\\nüìä R√©sultats :\")\n",
    "print(classification_report(y_test, y_pred_log_balanced, \n",
    "                          target_names=['Rest√©', 'Parti']))\n",
    "\n",
    "cm_log = confusion_matrix(y_test, y_pred_log_balanced)\n",
    "print(f\"\\nMatrice de confusion :\")\n",
    "print(cm_log)\n",
    "print(f\"\\n  TP (D√©parts d√©tect√©s) : {cm_log[1,1]}/{(y_test==1).sum()}\")\n",
    "print(f\"  FN (D√©parts manqu√©s) : {cm_log[1,0]}/{(y_test==1).sum()}\")\n",
    "\n",
    "recall_log = recall_score(y_test, y_pred_log_balanced)\n",
    "f1_log = f1_score(y_test, y_pred_log_balanced)\n",
    "print(f\"\\n  ‚Üí Recall : {recall_log:.2%}\")\n",
    "print(f\"  ‚Üí F1-Score : {f1_log:.2%}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. RANDOM FOREST avec class_weight\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"‚îÄ\"*80)\n",
    "print(\"üîÑ Mod√®le 2 : Random Forest + class_weight='balanced'\")\n",
    "print(\"‚îÄ\"*80)\n",
    "\n",
    "rf_balanced = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',  # ‚Üê IMPORTANT\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_balanced.fit(X_train, y_train)\n",
    "y_pred_rf_balanced = rf_balanced.predict(X_test)\n",
    "\n",
    "print(\"\\nüìä R√©sultats :\")\n",
    "print(classification_report(y_test, y_pred_rf_balanced, \n",
    "                          target_names=['Rest√©', 'Parti']))\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf_balanced)\n",
    "print(f\"\\nMatrice de confusion :\")\n",
    "print(cm_rf)\n",
    "print(f\"\\n  TP (D√©parts d√©tect√©s) : {cm_rf[1,1]}/{(y_test==1).sum()}\")\n",
    "print(f\"  FN (D√©parts manqu√©s) : {cm_rf[1,0]}/{(y_test==1).sum()}\")\n",
    "\n",
    "recall_rf = recall_score(y_test, y_pred_rf_balanced)\n",
    "f1_rf = f1_score(y_test, y_pred_rf_balanced)\n",
    "print(f\"\\n  ‚Üí Recall : {recall_rf:.2%}\")\n",
    "print(f\"  ‚Üí F1-Score : {f1_rf:.2%}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. COMPARAISON AVANT/APR√àS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARAISON : AVANT vs APR√àS class_weight\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "resultats_comparaison = pd.DataFrame([\n",
    "    {\n",
    "        'Mod√®le': 'Logistic Regression (sans class_weight)',\n",
    "        'Recall': 0.3830,  # Votre r√©sultat pr√©c√©dent\n",
    "        'F1-Score': 0.5294,\n",
    "        'TP': 18,\n",
    "        'FN': 29\n",
    "    },\n",
    "    {\n",
    "        'Mod√®le': 'Logistic Regression (AVEC class_weight)',\n",
    "        'Recall': recall_log,\n",
    "        'F1-Score': f1_log,\n",
    "        'TP': cm_log[1,1],\n",
    "        'FN': cm_log[1,0]\n",
    "    },\n",
    "    {\n",
    "        'Mod√®le': 'Random Forest (AVEC class_weight)',\n",
    "        'Recall': recall_rf,\n",
    "        'F1-Score': f1_rf,\n",
    "        'TP': cm_rf[1,1],\n",
    "        'FN': cm_rf[1,0]\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + resultats_comparaison.to_string(index=False))\n",
    "\n",
    "# Identifier le meilleur\n",
    "meilleur_idx = resultats_comparaison['Recall'].idxmax()\n",
    "meilleur = resultats_comparaison.iloc[meilleur_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üèÜ MEILLEUR MOD√àLE : {meilleur['Mod√®le']}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n  ‚Ä¢ Recall : {meilleur['Recall']:.2%} ‚Üí D√©tecte {meilleur['Recall']:.0%} des d√©parts\")\n",
    "print(f\"  ‚Ä¢ F1-Score : {meilleur['F1-Score']:.2%}\")\n",
    "print(f\"  ‚Ä¢ D√©parts d√©tect√©s : {int(meilleur['TP'])}/{(y_test==1).sum()}\")\n",
    "print(f\"  ‚Ä¢ D√©parts manqu√©s : {int(meilleur['FN'])}/{(y_test==1).sum()}\")\n",
    "\n",
    "gain_recall = meilleur['Recall'] - 0.3830\n",
    "print(f\"\\n‚úÖ Gain de Recall : +{gain_recall:.1%} par rapport au mod√®le initial\")\n",
    "print(f\"   ‚Üí {int(meilleur['TP']) - 18} d√©parts suppl√©mentaires d√©tect√©s !\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ √âTAPE 4.1 TERMIN√âE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Les features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# √âTAPE 5 : FEATURE IMPORTANCE (VERSION SIMPLE - SANS SHAP)\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 5 : INTERPR√âTABILIT√â - FEATURE IMPORTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüí° Objectif : Identifier les CAUSES d'attrition chez TechNova\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. MOD√àLE FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"‚îÄ\"*80)\n",
    "print(\"üîÑ Entra√Ænement du mod√®le final\")\n",
    "print(\"‚îÄ\"*80)\n",
    "\n",
    "modele_final = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "modele_final.fit(X_train, y_train)\n",
    "print(\"‚úÖ Mod√®le entra√Æn√©\")\n",
    "\n",
    "# Performance\n",
    "y_pred_final = modele_final.predict(X_test)\n",
    "print(f\"\\nüìä Performance (TEST) :\")\n",
    "print(f\"  ‚Ä¢ Accuracy : {accuracy_score(y_test, y_pred_final):.2%}\")\n",
    "print(f\"  ‚Ä¢ Recall : {recall_score(y_test, y_pred_final):.2%}\")\n",
    "print(f\"  ‚Ä¢ F1-Score : {f1_score(y_test, y_pred_final):.2%}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. FEATURE IMPORTANCE (COEFFICIENTS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä ANALYSE DES COEFFICIENTS DU MOD√àLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extraire les coefficients\n",
    "coefficients = modele_final.coef_[0]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Abs_Coefficient': np.abs(coefficients)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "# TOP 20\n",
    "print(\"\\nüîù TOP 20 features les plus importantes :\")\n",
    "print(\"‚îÄ\"*80)\n",
    "\n",
    "for idx, (i, row) in enumerate(feature_importance.head(20).iterrows(), 1):\n",
    "    feature = row['Feature']\n",
    "    coef = row['Coefficient']\n",
    "    \n",
    "    # Interpr√©tation\n",
    "    if coef > 0:\n",
    "        impact = \"‚Üó AUGMENTE le risque de d√©part\"\n",
    "        emoji = \"üî¥\"\n",
    "    else:\n",
    "        impact = \"‚Üò DIMINUE le risque de d√©part (PROT√àGE)\"\n",
    "        emoji = \"üü¢\"\n",
    "    \n",
    "    print(f\"{idx:2d}. {feature[:50]:50s}\")\n",
    "    print(f\"    Coefficient : {coef:+.4f}  {emoji} {impact}\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. VISUALISATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìä G√©n√©ration des graphiques...\")\n",
    "\n",
    "# GRAPHIQUE 1 : TOP 20 coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "# Graphique 1a : Tous les coefficients (positifs et n√©gatifs)\n",
    "top_20 = feature_importance.head(20).sort_values('Coefficient')\n",
    "colors = ['#e74c3c' if x > 0 else '#2ecc71' for x in top_20['Coefficient']]\n",
    "\n",
    "axes[0].barh(range(len(top_20)), top_20['Coefficient'], \n",
    "             color=colors, edgecolor='black', linewidth=1)\n",
    "axes[0].set_yticks(range(len(top_20)))\n",
    "axes[0].set_yticklabels(top_20['Feature'], fontsize=9)\n",
    "axes[0].set_xlabel('Coefficient', fontsize=12)\n",
    "axes[0].set_title('TOP 20 Features - Impact sur l\\'attrition\\nüî¥ Rouge = Favorise d√©parts | üü¢ Vert = Prot√®ge contre d√©parts', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=1.5)\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Graphique 1b : Importance absolue\n",
    "top_20_abs = feature_importance.head(20).sort_values('Abs_Coefficient')\n",
    "axes[1].barh(range(len(top_20_abs)), top_20_abs['Abs_Coefficient'], \n",
    "             color='steelblue', edgecolor='black', linewidth=1)\n",
    "axes[1].set_yticks(range(len(top_20_abs)))\n",
    "axes[1].set_yticklabels(top_20_abs['Feature'], fontsize=9)\n",
    "axes[1].set_xlabel('Importance absolue', fontsize=12)\n",
    "axes[1].set_title('TOP 20 Features - Importance globale\\n(sans tenir compte du sens)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique 1/2 : Feature Importance\")\n",
    "\n",
    "# GRAPHIQUE 2 : Distribution des coefficients\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.hist(coefficients, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Coefficient = 0')\n",
    "ax.set_xlabel('Coefficient', fontsize=12)\n",
    "ax.set_ylabel('Nombre de features', fontsize=12)\n",
    "ax.set_title('Distribution des coefficients du mod√®le', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique 2/2 : Distribution des coefficients\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. INTERPR√âTATION M√âTIER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíº INTERPR√âTATION M√âTIER POUR TECHNOVA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüéØ TOP 5 CAUSES D'ATTRITION :\")\n",
    "print(\"‚îÄ\"*80)\n",
    "\n",
    "top_5 = feature_importance.head(5)\n",
    "\n",
    "for idx, (i, row) in enumerate(top_5.iterrows(), 1):\n",
    "    feature = row['Feature']\n",
    "    coef = row['Coefficient']\n",
    "    \n",
    "    print(f\"\\n{idx}. {feature}\")\n",
    "    print(f\"   Coefficient : {coef:+.4f}\")\n",
    "    \n",
    "    if coef > 0:\n",
    "        print(f\"   üí° Plus cette variable augmente, plus le risque de d√©part AUGMENTE\")\n",
    "        print(f\"   üéØ Action RH : Surveiller/am√©liorer ce facteur\")\n",
    "    else:\n",
    "        print(f\"   üí° Plus cette variable augmente, plus le risque de d√©part DIMINUE\")\n",
    "        print(f\"   üéØ Action RH : Renforcer ce facteur protecteur\")\n",
    "\n",
    "# Facteurs de risque vs protecteurs\n",
    "facteurs_risque = feature_importance[feature_importance['Coefficient'] > 0].head(5)\n",
    "facteurs_protection = feature_importance[feature_importance['Coefficient'] < 0].head(5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã SYNTH√àSE POUR LES RH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüî¥ TOP 5 FACTEURS DE RISQUE (augmentent les d√©parts) :\")\n",
    "for idx, (i, row) in enumerate(facteurs_risque.iterrows(), 1):\n",
    "    print(f\"  {idx}. {row['Feature'][:50]} (coef: {row['Coefficient']:+.4f})\")\n",
    "\n",
    "print(\"\\nüü¢ TOP 5 FACTEURS PROTECTEURS (r√©duisent les d√©parts) :\")\n",
    "for idx, (i, row) in enumerate(facteurs_protection.iterrows(), 1):\n",
    "    print(f\"  {idx}. {row['Feature'][:50]} (coef: {row['Coefficient']:+.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ √âTAPE 5 TERMIN√âE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüéâ MISSION ACCOMPLIE !\")\n",
    "print(\"  ‚úÖ Mod√®le optimis√© : Recall de 68%\")\n",
    "print(\"  ‚úÖ Causes d'attrition identifi√©es\")\n",
    "print(\"  ‚úÖ Recommandations pour les RH\")\n",
    "print(\"\\n‚è≠Ô∏è  Il ne reste plus que la pr√©sentation PowerPoint !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
